{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4ldu1bY/bxGctOm+NG2za",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kigit2017/PCA/blob/main/PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA ->  \n",
        "It is used for reducing the number of features in a dataset while preserving as much of the original dataset.PCA acheives this by transforming the original features into a new set of uncorrelated fearures, known as the principal componenet. This PC is the linear combination of the original features.\n",
        "\n",
        "#### Key Concepts in PCA:\n",
        "- 1. Variance : Variance measures the spread or dispersion of data points.in PCA, the goal is to preserve as much variance as possible while reducing the dimensionality of the data.\n",
        "- 2. Covariance :  it measures the degree to whihc two variables changes together.\n",
        "+ve covariance indicates that the variables tend to increase or decrease together,\n",
        "-ve indicates an iverse realtionship.\n",
        "\n",
        "#### Process in PCA\n",
        "- 1. Standarization : 1st step in PCA is standarizee the data, which means transforming it to have a mean of O and a standard deviation of 1 for each features.\n",
        "The step enseure that each feature contributes equally to the analysis, especially when features have different units or scales.\n",
        "\n",
        "- 2. Covariance Matrix : PCA claculates the covariance matrix of the standardized data.It represents the relationship among all paris of features.\n",
        "\n",
        "- 3. Eigen Value Decomposition : PCA calculates the covariance matrix of the standarized data. The covariance matrix represents the realtioship and varaince among all pairs of features.\n",
        "    - Eginvalue:  These represents amount of variance that each principal component captures larger eigenvalyes corresponds to principal componenet that captutre more variance.\n",
        "    - Eigenvector(Principal Components) : These are the direction in the original features space along which the data varies the most. Each eigenvector corresponds to our PCA.\n",
        "- 4. Selecting Principal Componenets :  PCA allows you to choose a subset of the principal componenets (typically fewer than the original features) to retain most of the variance in the data.\n",
        "You can decide the number of principal componenets to keep based on the explained varaince or domain-specific requirements.\n",
        "- 5. Projection : The selected principal componenet are used to project the original data onto a lower-dimesional subspace. This results in a new dataset with reduced dimesionality while retaining the most important information\n",
        "- 6. Reconstruction : You can recosntruct the original data from the reduced-dimensional representation  y reversing the projection. However , some information may be lost during the dimensionality reduction.\n",
        "\n",
        "#### Advantages of PCA:\n",
        "- Dimensionality Reduction : PCA is used to reduce the dimensionality of high-dimesional data, making it easier to visulaize and analyze.\n",
        "- Noise Reduction : By removin the uncessary components from our data and keeping only useful components help us to reduce the noise in our dataset.\n",
        "- Visulization : PCA can be used to project data into lower dimension, allowing for easier visulization and interpretation\n",
        "- Feature Engineering : In some cases, PCA can be used for feature engineering by creating new features that capture the most important information in the data.\n",
        "\n",
        "\n",
        "PCA is not ML model, it is a dimsionality reduction technique while preprocessing  step in variousML and data analysis task.\n",
        "It applies on the input data to reduce its dimensionality before feedin it into a machine learning model.\n",
        "\n",
        "### PCA helps in  Anomaly Detection Models\n",
        "- Isolation Forest\n",
        "- One-class SVM\n",
        "- Autoencoders\n",
        "\n"
      ],
      "metadata": {
        "id": "y6Sxb9uIboo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gLKuZU0bbol5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sVem95_ybOv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa01f6b-eecf-4fb6-a901-5a51e7d37553"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "iris = load_iris()\n",
        "X,y = iris.data,iris.target\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 3)\n",
        "x_pca = pca.fit_transform(X)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_pca,y)\n",
        "y_pred = lr.predict(x_pca)\n",
        "#y_pred.shape\n",
        "y_pred=y_pred.reshape(-1,1)\n",
        "y_pred.shape\n",
        "# find the accuracy\n",
        "# logitic\n",
        "# SVC\n",
        "#\n",
        "#y_pred\n",
        "#accuracy = accuracy_score(y,np.round(y_pred))\n",
        "print(\"Accuracy\",accuracy)\n",
        "f1_score = f1_score(y,np.round(y_pred),average=None)\n"
      ],
      "metadata": {
        "id": "ZZwvncVhmtQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "siUoJC59rmDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "House Prediction -PCA"
      ],
      "metadata": {
        "id": "XRG44SATQbLR"
      }
    }
  ]
}